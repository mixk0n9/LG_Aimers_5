{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "017e9265",
   "metadata": {},
   "source": [
    "# 제품 이상여부 판별 프로젝트\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdab431",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8341e8",
   "metadata": {},
   "source": [
    "### 필수 라이브러리\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "791d7f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: hyperopt in ./.local/lib/python3.10/site-packages (0.2.7)\n",
      "Requirement already satisfied: lightgbm in ./.local/lib/python3.10/site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from hyperopt) (1.23.5)\n",
      "Requirement already satisfied: scipy in ./.local/lib/python3.10/site-packages (from hyperopt) (1.14.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (from hyperopt) (4.66.4)\n",
      "Requirement already satisfied: future in ./.local/lib/python3.10/site-packages (from hyperopt) (1.0.0)\n",
      "Requirement already satisfied: networkx>=2.2 in ./.local/lib/python3.10/site-packages (from hyperopt) (3.2.1)\n",
      "Requirement already satisfied: six in ./.local/lib/python3.10/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: py4j in ./.local/lib/python3.10/site-packages (from hyperopt) (0.10.9.7)\n",
      "Requirement already satisfied: cloudpickle in ./.local/lib/python3.10/site-packages (from hyperopt) (3.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hyperopt lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a315cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=(SettingWithCopyWarning))\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d054e30",
   "metadata": {},
   "source": [
    "### 데이터 읽어오기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc0b4d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = \"./data/\"\n",
    "train = pd.read_csv(os.path.join(ROOT_DIR, \"train.csv\"))\n",
    "test = pd.read_csv(os.path.join(ROOT_DIR, \"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14829404",
   "metadata": {},
   "source": [
    "X Judge Value_Dam -> X Collect Result가 NaN이면 NG, 숫자면 NaN으로 결측치 보간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74532db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam' 컬럼의 결측치를 조건에 따라 채우기   -> 이거 안돼있는데??\n",
    "train['HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam'] = train['HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam'].combine_first(\n",
    "    train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].apply(lambda x: 'NG' if pd.isna(x) else np.nan)\n",
    ")\n",
    "\n",
    "test['HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam'] = test['HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam'].combine_first(\n",
    "    test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].apply(lambda x: 'NG' if pd.isna(x) else np.nan)\n",
    ")\n",
    "\n",
    "fill_nan = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', \n",
    "            'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', \n",
    "           'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2']\n",
    "for col in fill_nan:\n",
    "  train[col] = train[col].replace('OK', np.nan)\n",
    "  test[col] = test[col].replace('OK', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a358d5",
   "metadata": {},
   "source": [
    "전부 NA이거나 unique값이 1이면서 NA가 없는 컬럼들 drop. OK만 들어가 있는 변수들은 Judge value Dam과 모두 겹치므로 drop.\n",
    "\n",
    "남는 컬럼 : 147개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5427b79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 컬럼 수 : 464\n",
      "drop할 컬럼 수 : 317\n",
      "사용 컬럼 수 : 147\n"
     ]
    }
   ],
   "source": [
    "drop_col = []\n",
    "\n",
    "for col in train.columns:\n",
    "  nullcount = train[col].isnull().sum()\n",
    "  nunique = train[col].nunique()\n",
    "  if nullcount == len(train):\n",
    "    drop_col.append(col)\n",
    "  if (nunique==1) & (nullcount == 0):\n",
    "    drop_col.append(col)\n",
    "\n",
    "# 겹치는 OK변수들 제거\n",
    "drop_col.extend(['HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill1',\n",
    "                 'HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill2',\n",
    "                 'GMES_ORIGIN_INSP_JUDGE_CODE Collect Result_AutoClave',\n",
    "                 'GMES_ORIGIN_INSP_JUDGE_CODE Judge Value_AutoClave']\n",
    "                 )\n",
    "# drop column\n",
    "print('전체 컬럼 수 :', len(train.columns))\n",
    "print('drop할 컬럼 수 :', len(drop_col))\n",
    "print('사용 컬럼 수 :', len(train.columns) - len(drop_col))\n",
    "\n",
    "train = train.drop(drop_col, axis=1)\n",
    "test = test.drop(drop_col, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6118ae22",
   "metadata": {},
   "source": [
    "### (X,Y,Z) 좌표 변수 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c352e9",
   "metadata": {},
   "source": [
    "head 공정 X, Y, Z 축 별로 연결 (dam, fill1, fill2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0eff3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "head_coordinate_col = [[\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam',\n",
    " 'HEAD Standby Position X Collect Result_Dam',\n",
    " 'HEAD Standby Position Y Collect Result_Dam',\n",
    " 'HEAD Standby Position Z Collect Result_Dam',\n",
    " 'Head Clean Position X Collect Result_Dam',\n",
    " 'Head Clean Position Y Collect Result_Dam',\n",
    " 'Head Clean Position Z Collect Result_Dam',\n",
    " 'Head Purge Position X Collect Result_Dam',\n",
    " 'Head Purge Position Y Collect Result_Dam',\n",
    " 'Head Purge Position Z Collect Result_Dam',\n",
    " 'Head Zero Position X Collect Result_Dam',\n",
    " 'Head Zero Position Y Collect Result_Dam',\n",
    " 'Head Zero Position Z Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
    " 'HEAD Standby Position X Collect Result_Fill1',\n",
    " 'HEAD Standby Position Y Collect Result_Fill1',\n",
    " 'HEAD Standby Position Z Collect Result_Fill1',\n",
    " 'Head Clean Position X Collect Result_Fill1',\n",
    " 'Head Clean Position Y Collect Result_Fill1',\n",
    " 'Head Clean Position Z Collect Result_Fill1',\n",
    " 'Head Purge Position X Collect Result_Fill1',\n",
    " 'Head Purge Position Y Collect Result_Fill1',\n",
    " 'Head Purge Position Z Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2',\n",
    " 'HEAD Standby Position X Collect Result_Fill2',\n",
    " 'HEAD Standby Position Y Collect Result_Fill2',\n",
    " 'HEAD Standby Position Z Collect Result_Fill2',\n",
    " 'Head Clean Position X Collect Result_Fill2',\n",
    " 'Head Clean Position Y Collect Result_Fill2',\n",
    " 'Head Clean Position Z Collect Result_Fill2',\n",
    " 'Head Purge Position X Collect Result_Fill2',\n",
    " 'Head Purge Position Y Collect Result_Fill2',\n",
    " 'Head Purge Position Z Collect Result_Fill2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e8f9193",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_col = head_coordinate_col[0].copy()\n",
    "\n",
    "grouped_columns = {}\n",
    "grouped_columns['Dam'] = {}\n",
    "grouped_columns['Fill1'] = {}\n",
    "grouped_columns['Fill2'] = {}\n",
    "\n",
    "# 그룹핑\n",
    "for column in pos_col:\n",
    "  if 'judge' not in column.lower():\n",
    "    parts = column.split('_')\n",
    "    if len(parts) == 2:\n",
    "        coord_info, process_type = parts\n",
    "        axis = 'X' if ' X ' in coord_info else 'Y' if ' Y ' in coord_info else 'Z' if ' Z ' in coord_info else 'Θ' if ' Θ ' in coord_info else 'Unknown'\n",
    "        # axis가 존재하지 않으면 새로 생성\n",
    "        if axis not in grouped_columns[process_type]:\n",
    "            grouped_columns[process_type][axis] = []\n",
    "        grouped_columns[process_type][axis].append(column)\n",
    "\n",
    "# 딕셔너리의 각 프로세스와 축에 대해 새로운 컬럼 생성\n",
    "for process_type, axes in grouped_columns.items():\n",
    "    for axis, columns in axes.items():\n",
    "        # 각 축에 대한 컬럼을 tuple로 묶어 새로운 컬럼을 생성\n",
    "        new_column_name = f'{process_type} {axis}'  # ex: 'Dam X', 'Fill1 X'\n",
    "        train[new_column_name] = list(zip(*[train[col] for col in columns]))\n",
    "        train[new_column_name] = train[new_column_name].astype(str)\n",
    "        test[new_column_name] = list(zip(*[test[col] for col in columns]))\n",
    "        test[new_column_name] = test[new_column_name].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b75d001",
   "metadata": {},
   "source": [
    "head 공정 중 clean - purge X,Y,Z 축 별로 연결 (Dam, Fill1 만)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4e75101",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanpurge_col_set = [['Head Clean Position X Collect Result_Fill1',\n",
    "'Head Purge Position X Collect Result_Fill1'],\n",
    "                 ['Head Clean Position Y Collect Result_Fill1',\n",
    "'Head Purge Position Y Collect Result_Fill1'],\n",
    "                 ['Head Clean Position Z Collect Result_Fill1',\n",
    "'Head Purge Position Z Collect Result_Fill1'],\n",
    "                 \n",
    "                 ['Head Clean Position X Collect Result_Dam',\n",
    "'Head Purge Position X Collect Result_Dam'],\n",
    "                 ['Head Clean Position Y Collect Result_Dam',\n",
    "'Head Purge Position Y Collect Result_Dam'],\n",
    "                 ['Head Clean Position Z Collect Result_Dam',\n",
    "'Head Purge Position Z Collect Result_Dam']]\n",
    "\n",
    "new_col = []\n",
    "for col_set in cleanpurge_col_set:\n",
    "  colname = col_set[0].replace('Clean', 'Clean&Purge')\n",
    "  new_col.append(colname)\n",
    "  train[colname] = list(train[col_set].itertuples(index=False, name=None))\n",
    "  train[colname] = train[colname].astype(str)\n",
    "  test[colname] = list(test[col_set].itertuples(index=False, name=None))\n",
    "  test[colname] = test[colname].astype(str)\n",
    "\n",
    "# drop_col = list(set(drop_col + sum(stage_col_set, [])))\n",
    "# len(drop_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cda678d",
   "metadata": {},
   "source": [
    "cure 공정 연결가능한 축끼리 연결 (Dam은 X,Θ / Fill2는 X,Z)\n",
    "\n",
    "없는 좌표는 nunique가 하나라 위에서 drop 당한거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f127ba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "cure_col_set = [ ['CURE START POSITION X Collect Result_Dam',\n",
    "                  'CURE END POSITION X Collect Result_Dam'],\n",
    "                 ['CURE START POSITION Θ Collect Result_Dam',\n",
    "                  'CURE END POSITION Θ Collect Result_Dam'],\n",
    "                 [ 'CURE START POSITION X Collect Result_Fill2',\n",
    "                   'CURE END POSITION X Collect Result_Fill2'],\n",
    "                 [ 'CURE START POSITION Z Collect Result_Fill2',\n",
    "                   'CURE END POSITION Z Collect Result_Fill2']]\n",
    "\n",
    "\n",
    "\n",
    "new_col = []\n",
    "for col_set in cure_col_set:\n",
    "  colname = col_set[0].replace('START', '')\n",
    "  new_col.append(colname)\n",
    "  train[colname] = list(train[col_set].itertuples(index=False, name=None))\n",
    "  train[colname] = train[colname].astype(str)\n",
    "  test[colname] = list(test[col_set].itertuples(index=False, name=None))\n",
    "  test[colname] = test[colname].astype(str)\n",
    "\n",
    "# drop_col = list(set(drop_col + sum(stage_col_set, [])))\n",
    "# len(drop_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae943cf",
   "metadata": {},
   "source": [
    "head 공정 stage 123 X,Y,Z 축기리 연결 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "513a1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_col_set = [['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
    "                  'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
    "                  'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'],\n",
    "                 ['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
    "                  'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
    "                  'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'],\n",
    "                 ['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam',\n",
    "                  'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
    "                  'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'],\n",
    "                 ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    "                  'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    "                  'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'],\n",
    "                 ['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "                  'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    "                  'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'],\n",
    "                 ['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    "                  'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    "                  'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'],\n",
    "                 ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
    "                  'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
    "                  'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2'],\n",
    "                 ['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2',\n",
    "                  'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2',\n",
    "                  'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2'],\n",
    "                 ['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2',\n",
    "                  'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2',\n",
    "                  'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2']]\n",
    "new_col = []\n",
    "for col_set in stage_col_set:\n",
    "  colname = col_set[0].replace('1)', ')')\n",
    "  new_col.append(colname)\n",
    "  train[colname] = list(train[col_set].itertuples(index=False, name=None))\n",
    "  train[colname] = train[colname].astype(str)\n",
    "  test[colname] = list(test[col_set].itertuples(index=False, name=None))\n",
    "  test[colname] = test[colname].astype(str)\n",
    "\n",
    "# drop_col = list(set(drop_col + sum(stage_col_set, [])))\n",
    "# len(drop_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79724257",
   "metadata": {},
   "source": [
    "모든 head 공정 좌표에 대하여 (X,Y,Z) 좌표 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f45793d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "coordinate_groups = {}\n",
    "\n",
    "head_dam_XYZ_col = []\n",
    "head_fill1_XYZ_col = []\n",
    "head_fill2_XYZ_col = []\n",
    "\n",
    "cure_XYZ_col = []\n",
    "\n",
    "# filtered_columns = [col for col in column_names if 'CURE' not in col]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for col in pos_col:\n",
    "    # 정규 표현식을 사용하여 X, Y, Z 부분을 찾고, 공통 이름을 추출\n",
    "    match = re.match(r'(.*?)(\\bX\\b|\\bY\\b|\\bZ\\b)(.*)', col)\n",
    "    if match:\n",
    "        base_name = match.group(1).strip() + ' ' + match.group(3).strip()\n",
    "        if base_name not in coordinate_groups:\n",
    "            coordinate_groups[base_name] = {}\n",
    "        coordinate_groups[base_name][match.group(2)] = col\n",
    "\n",
    "# 각 그룹에 대해 가능한 X, Y, Z 좌표를 묶어서 새로운 컬럼 추가\n",
    "for base_name, coords in coordinate_groups.items():\n",
    "    # 기존 컬럼 중 존재하는 X, Y, Z 값을 가져와서 묶기\n",
    "    grouped_columns_train = []\n",
    "    grouped_columns_test = []\n",
    "    for coord in ['X', 'Y', 'Z']:\n",
    "        if coord in coords:\n",
    "            grouped_columns_train.append(train[coords[coord]])\n",
    "            grouped_columns_test.append(test[coords[coord]])\n",
    "        else:\n",
    "            grouped_columns_train.append([None] * len(train))  # 해당 컬럼이 없으면 None으로 채움\n",
    "            grouped_columns_test.append([None] * len(test))\n",
    "    # 새로운 (X, Y, Z) 튜플 컬럼 생성\n",
    "    train[f'{base_name} XYZ'] = [str(tup) for tup in list(zip(*grouped_columns_train))] \n",
    "    test[f'{base_name} XYZ'] = [str(tup) for tup in list(zip(*grouped_columns_test))]\n",
    "    if 'head' in base_name.lower():\n",
    "        if 'dam' in base_name.lower():\n",
    "            head_dam_XYZ_col.append(f'{base_name} XYZ')\n",
    "        if 'fill1' in base_name.lower():\n",
    "            head_fill1_XYZ_col.append(f'{base_name} XYZ')\n",
    "        if 'fill2' in base_name.lower():\n",
    "            head_fill2_XYZ_col.append(f'{base_name} XYZ')\n",
    "    \n",
    "            \n",
    "    # else:\n",
    "    #     cure_XYZ_col.append(f'{base_name} XYZ')\n",
    "    \n",
    "    # 기존의 X, Y, Z 컬럼 제거\n",
    "    train.drop(columns=list(coords.values()), inplace=True)\n",
    "    test.drop(columns=list(coords.values()), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afd585",
   "metadata": {},
   "source": [
    "모든 cure 공정 좌표에 대하여 X_Z_Θ 좌표 생성 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b74faf4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CURE END POSITION X Collect Result_Dam',\n",
       " 'CURE END POSITION Z Collect Result_Dam',\n",
       " 'CURE END POSITION Θ Collect Result_Dam',\n",
       " 'CURE START POSITION X Collect Result_Dam',\n",
       " 'CURE START POSITION Θ Collect Result_Dam',\n",
       " 'CURE END POSITION X Collect Result_Fill2',\n",
       " 'CURE END POSITION Z Collect Result_Fill2',\n",
       " 'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
       " 'CURE START POSITION X Collect Result_Fill2',\n",
       " 'CURE START POSITION Z Collect Result_Fill2']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cure_coordinate_col = [col for col in train.columns if 'CURE' in col]\n",
    "cure_coordinate_col = [col for col in cure_coordinate_col if 'SPEED' not in col]\n",
    "\n",
    "cure_coordinate_col = cure_coordinate_col[:-4]\n",
    "\n",
    "train[cure_coordinate_col] = train[cure_coordinate_col].astype(str)\n",
    "test[cure_coordinate_col] = test[cure_coordinate_col].astype(str)\n",
    "\n",
    "cure_coordinate_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "486c39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['CURE START POSITION Collect Result_Dam XΘ'] = train['CURE START POSITION X Collect Result_Dam'] + '_' + train['CURE START POSITION Θ Collect Result_Dam']\n",
    "train['CURE END POSITION Collect Result_Dam XZΘ'] = train['CURE END POSITION X Collect Result_Dam'] + '_' + train['CURE END POSITION Z Collect Result_Dam'] +  '_' + train['CURE END POSITION Θ Collect Result_Dam']\n",
    "\n",
    "train['CURE START POSITION Collect Result_Fill2 XZ'] = train['CURE START POSITION X Collect Result_Fill2'] + '_' + train['CURE START POSITION Z Collect Result_Fill2']\n",
    "train['CURE END POSITION Collect Result_Fill2 XZ'] = train['CURE END POSITION X Collect Result_Fill2'] + '_' + train['CURE END POSITION Z Collect Result_Fill2'] \n",
    "\n",
    "test['CURE START POSITION Collect Result_Dam XΘ'] = test['CURE START POSITION X Collect Result_Dam'] + '_' + test['CURE START POSITION Θ Collect Result_Dam']\n",
    "test['CURE END POSITION Collect Result_Dam XZΘ'] = test['CURE END POSITION X Collect Result_Dam'] + '_' + test['CURE END POSITION Z Collect Result_Dam'] +  '_' + test['CURE END POSITION Θ Collect Result_Dam']\n",
    "\n",
    "test['CURE START POSITION Collect Result_Fill2 XZ'] = test['CURE START POSITION X Collect Result_Fill2'] + '_' + test['CURE START POSITION Z Collect Result_Fill2']\n",
    "test['CURE END POSITION Collect Result_Fill2 XZ'] = test['CURE END POSITION X Collect Result_Fill2'] + '_' + test['CURE END POSITION Z Collect Result_Fill2'] \n",
    "\n",
    "\n",
    "\n",
    "train.drop(columns=cure_coordinate_col, inplace=True)\n",
    "\n",
    "test.drop(columns=cure_coordinate_col, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c060a66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HEAD NORMAL COORDINATE AXIS(Stage1) Collect Result_Dam XYZ',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage1) Judge Value_Dam XYZ',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage2) Collect Result_Dam XYZ',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage3) Collect Result_Dam XYZ',\n",
       " 'HEAD Standby Position Collect Result_Dam XYZ',\n",
       " 'Head Clean Position Collect Result_Dam XYZ',\n",
       " 'Head Purge Position Collect Result_Dam XYZ',\n",
       " 'Head Zero Position Collect Result_Dam XYZ']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_dam_XYZ_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cedd3f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "del head_dam_XYZ_col[1]  #Judge 제외\n",
    "del head_dam_XYZ_col[-1] # Zero 제외\n",
    " \n",
    "\n",
    "head_XYZ_col = head_dam_XYZ_col + head_fill1_XYZ_col + head_fill2_XYZ_col \n",
    "\n",
    "head_dam_XYZ_col = [head_dam_XYZ_col[3]] + head_dam_XYZ_col[:3] + head_dam_XYZ_col[4:]\n",
    "head_fill1_XYZ_col = [head_fill1_XYZ_col[3]] + head_fill1_XYZ_col[:3] + head_fill1_XYZ_col[4:]\n",
    "head_fill2_XYZ_col = [head_fill2_XYZ_col[3]] + head_fill2_XYZ_col[:3] + head_fill2_XYZ_col[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6209c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cure_XYZ_col = train.columns[-4:].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b39e51ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HEAD Standby Position Collect Result_Dam XYZ',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage1) Collect Result_Dam XYZ',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage2) Collect Result_Dam XYZ',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage3) Collect Result_Dam XYZ',\n",
       " 'Head Clean Position Collect Result_Dam XYZ',\n",
       " 'Head Purge Position Collect Result_Dam XYZ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_dam_XYZ_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0c9ee44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HEAD Standby Position Collect Result_Dam XYZ',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage1) Collect Result_Dam XYZ',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage2) Collect Result_Dam XYZ',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage3) Collect Result_Dam XYZ',\n",
       " 'Head Clean Position Collect Result_Dam XYZ',\n",
       " 'Head Purge Position Collect Result_Dam XYZ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_dam_XYZ_col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8929f87",
   "metadata": {},
   "source": [
    "head interaction: head 공정 Stage1,2,3,Standby,Clean,Purge (X,Y,Z) 전체연결 (Dam,Fill1,Fill2 분리 X)\n",
    "\n",
    "stage123 : head 공정 중 stage123만 (X,Y,Z) 연결 (Dam,Fill1,Fill2 분리 O)\n",
    "\n",
    "clean_purge: head 공정 중 clean_purge만 (X,Y,Z) 연결 (Dam,Fill1,Fill2 분리 O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "661a6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 1. 라벨 인코딩 적용\n",
    "label_encoders = {}\n",
    "for col in head_XYZ_col:\n",
    "    le = LabelEncoder()\n",
    "    train[col] = le.fit_transform(train[col])\n",
    "    for label in list(test[col].unique()): #train에 없는 test값 처리\n",
    "        if label not in list(le.classes_):\n",
    "            le.classes_ = np.append(le.classes_,label)\n",
    "    test[col] = le.transform(test[col])\n",
    "    label_encoders[col] = le\n",
    "    train[col] = train[col].astype(str) # 범주 변수 처리\n",
    "    test[col] = test[col].astype(str) # 범주 변수 처리\n",
    "    \n",
    "# 2. 전체 상호작용항 생성 및 라벨 인코딩\n",
    "interaction_col_name = 'head_interaction'  # 향후에 head_interaction dam/fill1/fill2로 분리해 볼 필요 있음\n",
    "\n",
    "train[interaction_col_name] = train[head_XYZ_col].astype(str).agg('_'.join, axis=1)\n",
    "test[interaction_col_name] = test[head_XYZ_col].astype(str).agg('_'.join, axis=1)\n",
    "\n",
    "# 2. 순차적 상호작용\n",
    "\n",
    "train['stage123_dam'] = train[head_dam_XYZ_col].iloc[:,1] + '_' + train[head_dam_XYZ_col].iloc[:,2] + '_' + train[head_dam_XYZ_col].iloc[:,3]\n",
    "train['clean_purge_dam'] = train[head_dam_XYZ_col].iloc[:,4] + '_' + train[head_dam_XYZ_col].iloc[:,4] \n",
    "\n",
    "train['stage123_fill1'] = train[head_fill1_XYZ_col].iloc[:,1] + '_' + train[head_fill1_XYZ_col].iloc[:,2] + '_' + train[head_fill1_XYZ_col].iloc[:,3]\n",
    "train['clean_purge_fill1'] = train[head_fill1_XYZ_col].iloc[:,4] + '_' + train[head_fill1_XYZ_col].iloc[:,5] \n",
    "\n",
    "train['stage123_fill2'] = train[head_fill2_XYZ_col].iloc[:,1] + '_' + train[head_fill2_XYZ_col].iloc[:,2] + '_' + train[head_fill2_XYZ_col].iloc[:,3]\n",
    "train['clean_purge_fill2'] = train[head_fill2_XYZ_col].iloc[:,4] + '_' + train[head_fill2_XYZ_col].iloc[:,5] \n",
    "\n",
    "####################################################################################################################3\n",
    "\n",
    "test['stage123_dam'] = test[head_dam_XYZ_col].iloc[:,1] + '_' + test[head_dam_XYZ_col].iloc[:,2] + '_' + test[head_dam_XYZ_col].iloc[:,3]\n",
    "test['clean_purge_dam'] = test[head_dam_XYZ_col].iloc[:,4] + '_' + test[head_dam_XYZ_col].iloc[:,4] \n",
    "test['stage123_fill1'] = test[head_fill1_XYZ_col].iloc[:,1] + '_' + test[head_fill1_XYZ_col].iloc[:,2] + '_' + test[head_fill1_XYZ_col].iloc[:,3]\n",
    "test['clean_purge_fill1'] = test[head_fill1_XYZ_col].iloc[:,4] + '_' + test[head_fill1_XYZ_col].iloc[:,5] \n",
    "\n",
    "test['stage123_fill2'] = test[head_fill2_XYZ_col].iloc[:,1] + '_' + test[head_fill2_XYZ_col].iloc[:,2] + '_' + test[head_fill2_XYZ_col].iloc[:,3]\n",
    "test['clean_purge_fill2'] = test[head_fill2_XYZ_col].iloc[:,4] + '_' + test[head_fill2_XYZ_col].iloc[:,5] \n",
    "\n",
    "\n",
    "# 4. 상호작용항에 라벨 인코딩 적용\n",
    "ordered_interaction_col_name = train.columns[train.columns.str.contains('dam|fill1|fill2')]\n",
    "\n",
    "for i in ordered_interaction_col_name:\n",
    "\n",
    "    interaction_le = LabelEncoder()\n",
    "    train[i] = interaction_le.fit_transform(train[i])\n",
    "    for label in list(test[i].unique()): #train에 없는 test값 처리\n",
    "        if label not in list(interaction_le.classes_):\n",
    "            interaction_le.classes_ = np.append(interaction_le.classes_,label)\n",
    "    test[i] = interaction_le.transform(test[i])\n",
    "    train[i] = train[i].astype(\"object\") # 범주 변수 처리\n",
    "    test[i] = test[i].astype(\"object\") # 범주 변수 처리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d1c9f",
   "metadata": {},
   "source": [
    "cure interaction: cure 공정 start,end X_Z_Θ 전체연결 (Dam,Fill2 분리 X)\n",
    "\n",
    "start_end : cure 공정 중 start,end 만 X_Z_Θ 연결 (Dam,Fill2 분리 O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c625ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. 라벨 인코딩 적용\n",
    "label_encoders = {}\n",
    "for col in cure_XYZ_col:\n",
    "    le = LabelEncoder()\n",
    "    train[col] = le.fit_transform(train[col])\n",
    "    for label in list(test[col].unique()): #train에 없는 test값 처리\n",
    "        if label not in list(le.classes_):\n",
    "            le.classes_ = np.append(le.classes_,label)\n",
    "    test[col] = le.transform(test[col])\n",
    "    label_encoders[col] = le\n",
    "    train[col] = train[col].astype(str) # 범주 변수 처리\n",
    "    test[col] = test[col].astype(str) # 범주 변수 처리\n",
    "\n",
    "# 2. 상호작용항 생성 및 라벨 인코딩\n",
    "interaction_col_name = 'cure_interaction'\n",
    "train[interaction_col_name] = train[cure_XYZ_col].astype(str).agg('_'.join, axis=1) \n",
    "test[interaction_col_name] = test[cure_XYZ_col].astype(str).agg('_'.join, axis=1)\n",
    "\n",
    "\n",
    "# 순차적 상호작용항\n",
    "\n",
    "train['start_end_dam'] = train[cure_XYZ_col].iloc[:,0] + '_' + train[cure_XYZ_col].iloc[:,1]\n",
    "# train['standby_start_fill2'] = train[cure_XYZ_col].iloc[:,3] + '_' + train[cure_XYZ_col].iloc[:,4]\n",
    "train['start_end_fill2'] = train[cure_XYZ_col].iloc[:,2] + '_' + train[cure_XYZ_col].iloc[:,3]\n",
    "\n",
    "test['start_end_dam'] = test[cure_XYZ_col].iloc[:,0] + '_' + test[cure_XYZ_col].iloc[:,1]\n",
    "# test['standby_start_fill2'] = test[cure_XYZ_col].iloc[:,3] + '_' + test[cure_XYZ_col].iloc[:,4]\n",
    "test['start_end_fill2'] = test[cure_XYZ_col].iloc[:,2] + '_' + test[cure_XYZ_col].iloc[:,3]\n",
    "\n",
    "\n",
    "# 3. 조합의 count 개수가 10 이하인 경우 하나의 라벨로 묶기\n",
    "#interaction_counts = train[interaction_col_name].value_counts()\n",
    "\n",
    "# 빈도수가 10 이하인 조합들을 \"Rare\"로 대체\n",
    "#rare_labels = interaction_counts[interaction_counts <= 10].index\n",
    "#train[interaction_col_name] = train[interaction_col_name].apply(lambda x: 'Rare' if x in rare_labels else x)\n",
    "#test[interaction_col_name] = test[interaction_col_name].apply(lambda x: 'Rare' if x in rare_labels else x)\n",
    "\n",
    "# 4. 상호작용항에 라벨 인코딩 적용\n",
    "#interaction_col_name = ['start_end_dam', 'standby_start_fill2', 'start_end_fill2']\n",
    "ordered_interaction_col_name = ['start_end_dam', 'start_end_fill2']\n",
    "\n",
    "for i in ordered_interaction_col_name:\n",
    "\n",
    "    interaction_le = LabelEncoder()\n",
    "    train[i] = interaction_le.fit_transform(train[i])\n",
    "    for label in list(test[i].unique()): #train에 없는 test값 처리\n",
    "        if label not in list(interaction_le.classes_):\n",
    "            interaction_le.classes_ = np.append(interaction_le.classes_,label)\n",
    "    test[i] = interaction_le.transform(test[i])\n",
    "    train[i] = train[i].astype(\"object\") # 범주 변수 처리\n",
    "    test[i] = test[i].astype(\"object\") # 범주 변수 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd5c2c9",
   "metadata": {},
   "source": [
    "모든 개별 (X,Y,Z) , X_Z_Θ 좌표 drop (상호작용 칼럼만 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12c0612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(columns=head_XYZ_col, inplace=True)\n",
    "test.drop(columns=head_XYZ_col, inplace=True)\n",
    "\n",
    "train.drop(columns=cure_XYZ_col, inplace=True)\n",
    "test.drop(columns=cure_XYZ_col, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a264526",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Mismatch_Dam_Fill1'] = (train['Production Qty Collect Result_Fill1'] != train['Production Qty Collect Result_Dam']).astype(object)\n",
    "train['Mismatch_Dam_Fill2'] = (train['Production Qty Collect Result_Fill2'] != train['Production Qty Collect Result_Dam']).astype(object)\n",
    "train['Mismatch_Fill1_Fill2'] = (train['Production Qty Collect Result_Fill1'] != train['Production Qty Collect Result_Fill2']).astype(object)\n",
    "\n",
    "test['Mismatch_Dam_Fill1'] = (test['Production Qty Collect Result_Fill1'] != test['Production Qty Collect Result_Dam']).astype(object)\n",
    "test['Mismatch_Dam_Fill2'] = (test['Production Qty Collect Result_Fill2'] != test['Production Qty Collect Result_Dam']).astype(object)\n",
    "test['Mismatch_Fill1_Fill2'] = (test['Production Qty Collect Result_Fill1'] != test['Production Qty Collect Result_Fill2']).astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "368cd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Mismatch_Receip_Dam_Fill1'] = (train['Receip No Collect Result_Fill1'] != train['Receip No Collect Result_Dam']).astype(object)\n",
    "train['Mismatch_Receip_Dam_Fill2'] = (train['Receip No Collect Result_Fill2'] != train['Receip No Collect Result_Dam']).astype(object)\n",
    "train['Mismatch_Receip_Fill1_Fill2'] = (train['Receip No Collect Result_Fill1'] != train['Receip No Collect Result_Fill2']).astype(object)\n",
    "\n",
    "test['Mismatch_Receip_Dam_Fill1'] = (test['Receip No Collect Result_Fill1'] != test['Receip No Collect Result_Dam']).astype(object)\n",
    "test['Mismatch_Receip_Dam_Fill2'] = (test['Receip No Collect Result_Fill2'] != test['Receip No Collect Result_Dam']).astype(object)\n",
    "test['Mismatch_Receip_Fill1_Fill2'] = (test['Receip No Collect Result_Fill1'] != test['Receip No Collect Result_Fill2']).astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b5a38ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA 개수가 0보다 큰 컬럼들:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 컬럼의 NA 개수 계산\n",
    "na_counts = train.isna().sum()\n",
    "\n",
    "# NA 개수가 0보다 큰 컬럼 목록 필터링\n",
    "columns_with_na = na_counts[na_counts > 0].index.tolist()\n",
    "\n",
    "# 결과 출력\n",
    "print(\"NA 개수가 0보다 큰 컬럼들:\")\n",
    "columns_with_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63fc8b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA 개수가 0보다 큰 컬럼들:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['target']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 컬럼의 NA 개수 계산\n",
    "na_counts = test.isna().sum()\n",
    "\n",
    "# NA 개수가 0보다 큰 컬럼 목록 필터링\n",
    "columns_with_na = na_counts[na_counts > 0].index.tolist()\n",
    "\n",
    "# 결과 출력\n",
    "print(\"NA 개수가 0보다 큰 컬럼들:\")\n",
    "columns_with_na"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f17c9fe",
   "metadata": {},
   "source": [
    "### 전처리 및 파생변수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35caf6fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형 변수 수 :  59\n",
      "숫자형 변수 수 :  66\n"
     ]
    }
   ],
   "source": [
    "data_type = train.dtypes.reset_index()\n",
    "data_type.columns = ['col', 'type']\n",
    "cat_features = list(data_type[data_type['type']=='object']['col'])\n",
    "num_features = list(data_type[data_type['type']!='object']['col'])\n",
    "data_type['unique'] = data_type['col'].apply(lambda x : train[x].nunique())\n",
    "data_type['nullcount'] = data_type['col'].apply(lambda x : train[x].isnull().sum())\n",
    "print('범주형 변수 수 : ' ,len(cat_features))\n",
    "print('숫자형 변수 수 : ' ,len(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f663ddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = train.loc[:,num_features]\n",
    "original_cols = train_num.columns\n",
    "rank = np.linalg.matrix_rank(train_num.values)\n",
    "independent_cols = []\n",
    "for i in range(len(original_cols)):\n",
    "    sub_matrix = train_num[original_cols[:i+1]].values\n",
    "    if np.linalg.matrix_rank(sub_matrix) > len(independent_cols):\n",
    "        independent_cols.append(original_cols[i])\n",
    "train = train.loc[:,cat_features + independent_cols]\n",
    "test = test.loc[:,cat_features + independent_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d01194fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형 변수 수 :  59\n",
      "숫자형 변수 수 :  54\n"
     ]
    }
   ],
   "source": [
    "data_type = train.dtypes.reset_index()\n",
    "data_type.columns = ['col', 'type']\n",
    "cat_features = list(data_type[data_type['type']=='object']['col'])\n",
    "num_features = list(data_type[data_type['type']!='object']['col'])\n",
    "data_type['unique'] = data_type['col'].apply(lambda x : train[x].nunique())\n",
    "data_type['nullcount'] = data_type['col'].apply(lambda x : train[x].isnull().sum())\n",
    "print('범주형 변수 수 : ' ,len(cat_features))\n",
    "print('숫자형 변수 수 : ' ,len(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b3b6021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범주형 변수 수 :  61\n",
      "숫자형 변수 수 :  52\n"
     ]
    }
   ],
   "source": [
    "#Workmode 변수는 카테고리로 치는 것이 합당하므로 범주화\n",
    "cat_add = ['WorkMode Collect Result_Fill1', 'WorkMode Collect Result_Fill2']\n",
    "for col in cat_add:\n",
    "    train[col] = train[col].astype(object)\n",
    "    test[col] = test[col].astype(object)\n",
    "    cat_features.append(col)\n",
    "    num_features.remove(col)\n",
    "print('범주형 변수 수 : ' ,len(cat_features))\n",
    "print('숫자형 변수 수 : ' ,len(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "86f6735f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "#category로 취급할 것들\n",
    "print(len(data_type[(data_type['unique']<=10)&(data_type['type']!='object')])) #unique값이 10개 이하이면 -> category로 취급\n",
    "#cat_add = list(data_type[(data_type['unique']<=10)&(data_type['type']!='object')]['col'])\n",
    "\n",
    "# 위에랑 사실 겹쳐서 오류나는데 오류나기전까지 들어간 얘들을 일일이 넣음\n",
    "cat_add = [#'CURE END POSITION Θ Collect Result_Dam',\n",
    "            'CURE SPEED Collect Result_Dam',\n",
    "            'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
    "            'Stage1 Circle1 Distance Speed Collect Result_Dam',\n",
    "            'Stage1 Circle2 Distance Speed Collect Result_Dam',\n",
    "            'Stage1 Line1 Distance Speed Collect Result_Dam',\n",
    "            'Stage1 Line2 Distance Speed Collect Result_Dam',\n",
    "            'Stage1 Line4 Distance Speed Collect Result_Dam',\n",
    "            'Stage2 Circle1 Distance Speed Collect Result_Dam',\n",
    "            'Stage2 Circle2 Distance Speed Collect Result_Dam',\n",
    "            'Stage2 Line2 Distance Speed Collect Result_Dam',\n",
    "            'Stage2 Line3 Distance Speed Collect Result_Dam',\n",
    "            'Stage2 Line4 Distance Speed Collect Result_Dam',\n",
    "            'Stage3 Circle2 Distance Speed Collect Result_Dam',\n",
    "            'Stage3 Line2 Distance Speed Collect Result_Dam',\n",
    "            'Stage3 Line4 Distance Speed Collect Result_Dam',\n",
    "            'THICKNESS 1 Collect Result_Dam',\n",
    "            'THICKNESS 2 Collect Result_Dam',\n",
    "            'THICKNESS 3 Collect Result_Dam',\n",
    "            'WorkMode Collect Result_Dam',\n",
    "            '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
    "            '2nd Pressure Unit Time_AutoClave',\n",
    "            'DISCHARGED SPEED OF RESIN Collect Result_Fill1',]\n",
    "            #'WorkMode Collect Result_Fill1']\n",
    "for col in cat_add:\n",
    "  cat_features.append(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "052bb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['target']:\n",
    "    le = LabelEncoder()\n",
    "    le = le.fit(train[col])\n",
    "    train[col] = le.transform(train[col])\n",
    "    for label in list(test[col].unique()): #train에 없는 test값 처리\n",
    "        if label not in list(le.classes_):\n",
    "            le.classes_ = np.append(le.classes_,label)\n",
    "    test[col] = le.transform(test[col])\n",
    "cat_features.remove('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d093942",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = train['target']\n",
    "train = train.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2ef9f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cat_features:\n",
    "  train[col] = train[col].astype(\"category\")\n",
    "  test[col] = test[col].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cc01eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['target'] = target.apply(lambda x : 1 if x==0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "075396d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 변수선택 permutation\n",
    "\n",
    "# train.drop(columns=todrop, inplace=True)\n",
    "# test.drop(columns=todrop, inplace=True)\n",
    "\n",
    "# cat_features = [col for col in cat_features if col not in todrop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5d71ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5307948631547419, 1: 8.61823708206687}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuning_train, tuning_valid = train_test_split(train, test_size=0.3, random_state=0, stratify=train['target'])\n",
    "# 적합에 필요한 데이터 분리\n",
    "x_train = tuning_train.drop('target', axis=1)\n",
    "y_train = tuning_train['target']\n",
    "# 검증에 필요한 데이터 분리\n",
    "x_valid = tuning_valid.drop('target', axis=1)\n",
    "y_valid = tuning_valid['target']\n",
    "# set seed\n",
    "SEED = 0\n",
    "# class weight\n",
    "class_weights = compute_class_weight(classes=[0,1], y=tuning_train['target'], class_weight='balanced')\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f375afdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Equipment_Dam',\n",
       " 'Model.Suffix_Dam',\n",
       " 'Workorder_Dam',\n",
       " 'Model.Suffix_AutoClave',\n",
       " 'Workorder_AutoClave',\n",
       " 'Chamber Temp. Judge Value_AutoClave',\n",
       " 'Equipment_Fill1',\n",
       " 'Model.Suffix_Fill1',\n",
       " 'Workorder_Fill1',\n",
       " 'Equipment_Fill2',\n",
       " 'Model.Suffix_Fill2',\n",
       " 'Workorder_Fill2',\n",
       " 'Dam X',\n",
       " 'Dam Y',\n",
       " 'Dam Z',\n",
       " 'Fill1 X',\n",
       " 'Fill1 Y',\n",
       " 'Fill1 Z',\n",
       " 'Fill2 X',\n",
       " 'Fill2 Y',\n",
       " 'Fill2 Z',\n",
       " 'Head Clean&Purge Position X Collect Result_Fill1',\n",
       " 'Head Clean&Purge Position Y Collect Result_Fill1',\n",
       " 'Head Clean&Purge Position Z Collect Result_Fill1',\n",
       " 'Head Clean&Purge Position X Collect Result_Dam',\n",
       " 'Head Clean&Purge Position Y Collect Result_Dam',\n",
       " 'Head Clean&Purge Position Z Collect Result_Dam',\n",
       " 'CURE  POSITION X Collect Result_Dam',\n",
       " 'CURE  POSITION Θ Collect Result_Dam',\n",
       " 'CURE  POSITION X Collect Result_Fill2',\n",
       " 'CURE  POSITION Z Collect Result_Fill2',\n",
       " 'HEAD NORMAL COORDINATE X AXIS(Stage) Collect Result_Dam',\n",
       " 'HEAD NORMAL COORDINATE Y AXIS(Stage) Collect Result_Dam',\n",
       " 'HEAD NORMAL COORDINATE Z AXIS(Stage) Collect Result_Dam',\n",
       " 'HEAD NORMAL COORDINATE X AXIS(Stage) Collect Result_Fill1',\n",
       " 'HEAD NORMAL COORDINATE Y AXIS(Stage) Collect Result_Fill1',\n",
       " 'HEAD NORMAL COORDINATE Z AXIS(Stage) Collect Result_Fill1',\n",
       " 'HEAD NORMAL COORDINATE X AXIS(Stage) Collect Result_Fill2',\n",
       " 'HEAD NORMAL COORDINATE Y AXIS(Stage) Collect Result_Fill2',\n",
       " 'HEAD NORMAL COORDINATE Z AXIS(Stage) Collect Result_Fill2',\n",
       " 'HEAD NORMAL COORDINATE AXIS(Stage1) Judge Value_Dam XYZ',\n",
       " 'Head Zero Position Collect Result_Dam XYZ',\n",
       " 'head_interaction',\n",
       " 'stage123_dam',\n",
       " 'clean_purge_dam',\n",
       " 'stage123_fill1',\n",
       " 'clean_purge_fill1',\n",
       " 'stage123_fill2',\n",
       " 'clean_purge_fill2',\n",
       " 'cure_interaction',\n",
       " 'start_end_dam',\n",
       " 'start_end_fill2',\n",
       " 'Mismatch_Dam_Fill1',\n",
       " 'Mismatch_Dam_Fill2',\n",
       " 'Mismatch_Fill1_Fill2',\n",
       " 'Mismatch_Receip_Dam_Fill1',\n",
       " 'Mismatch_Receip_Dam_Fill2',\n",
       " 'Mismatch_Receip_Fill1_Fill2',\n",
       " 'WorkMode Collect Result_Fill1',\n",
       " 'WorkMode Collect Result_Fill2',\n",
       " 'CURE SPEED Collect Result_Dam',\n",
       " 'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
       " 'Stage1 Circle1 Distance Speed Collect Result_Dam',\n",
       " 'Stage1 Circle2 Distance Speed Collect Result_Dam',\n",
       " 'Stage1 Line1 Distance Speed Collect Result_Dam',\n",
       " 'Stage1 Line2 Distance Speed Collect Result_Dam',\n",
       " 'Stage1 Line4 Distance Speed Collect Result_Dam',\n",
       " 'Stage2 Circle1 Distance Speed Collect Result_Dam',\n",
       " 'Stage2 Circle2 Distance Speed Collect Result_Dam',\n",
       " 'Stage2 Line2 Distance Speed Collect Result_Dam',\n",
       " 'Stage2 Line3 Distance Speed Collect Result_Dam',\n",
       " 'Stage2 Line4 Distance Speed Collect Result_Dam',\n",
       " 'Stage3 Circle2 Distance Speed Collect Result_Dam',\n",
       " 'Stage3 Line2 Distance Speed Collect Result_Dam',\n",
       " 'Stage3 Line4 Distance Speed Collect Result_Dam',\n",
       " 'THICKNESS 1 Collect Result_Dam',\n",
       " 'THICKNESS 2 Collect Result_Dam',\n",
       " 'THICKNESS 3 Collect Result_Dam',\n",
       " 'WorkMode Collect Result_Dam',\n",
       " '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
       " '2nd Pressure Unit Time_AutoClave',\n",
       " 'DISCHARGED SPEED OF RESIN Collect Result_Fill1']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ecfa9b",
   "metadata": {},
   "source": [
    "## 3. 모델 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "913dc794",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "    'num_leaves': hp.quniform('num_leaves', 20, 150, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 5, 30, 1),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'n_estimators': 1000,\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0)\n",
    "}\n",
    "\n",
    "def f1_eval_metric(y_true, y_pred):\n",
    "    # LightGBM은 예측 확률로 전달하므로, 이를 이진 예측으로 변환해야 합니다.\n",
    "    y_pred_binary = np.round(y_pred)  # 0.5 기준으로 이진화\n",
    "    f1 = f1_score(y_true, y_pred_binary)\n",
    "    return 'f1_score', f1, True  \n",
    "\n",
    "# 목적 함수 정의\n",
    "def objective(params):\n",
    "    params['num_leaves'] = int(params['num_leaves'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "\n",
    "    model = lgb.LGBMClassifier(**params, class_weight = class_weights, random_state=SEED, verbose=-1)\n",
    "    model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric=f1_eval_metric,\n",
    "              categorical_feature = cat_features,\n",
    "              callbacks = [lgb.early_stopping(stopping_rounds = 100), lgb.log_evaluation(period = 0)])\n",
    "\n",
    "    preds = model.predict(x_valid)\n",
    "    score = f1_score(y_valid, preds)\n",
    "\n",
    "    # Hyperopt가 최소화를 수행하므로, 음수의 accuracy를 반환합니다.\n",
    "    return {'loss': -score, 'status': STATUS_OK}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7713ed1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:                     \n",
      "[74]\tvalid_0's binary_logloss: 0.347291\tvalid_0's f1_score: 0.183521\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[42]\tvalid_0's binary_logloss: 0.348984\tvalid_0's f1_score: 0.191717\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[87]\tvalid_0's binary_logloss: 0.314638\tvalid_0's f1_score: 0.207668\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[188]\tvalid_0's binary_logloss: 0.38227\tvalid_0's f1_score: 0.204931\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[114]\tvalid_0's binary_logloss: 0.312558\tvalid_0's f1_score: 0.210137\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[35]\tvalid_0's binary_logloss: 0.373764\tvalid_0's f1_score: 0.181704\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[45]\tvalid_0's binary_logloss: 0.34722\tvalid_0's f1_score: 0.202476\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[128]\tvalid_0's binary_logloss: 0.346596\tvalid_0's f1_score: 0.201817\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[56]\tvalid_0's binary_logloss: 0.393584\tvalid_0's f1_score: 0.192711\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[149]\tvalid_0's binary_logloss: 0.309899\tvalid_0's f1_score: 0.201111\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[133]\tvalid_0's binary_logloss: 0.374999\tvalid_0's f1_score: 0.200396\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[43]\tvalid_0's binary_logloss: 0.342178\tvalid_0's f1_score: 0.177725\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[71]\tvalid_0's binary_logloss: 0.305453\tvalid_0's f1_score: 0.187824\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[214]\tvalid_0's binary_logloss: 0.287509\tvalid_0's f1_score: 0.201917\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[20]\tvalid_0's binary_logloss: 0.354585\tvalid_0's f1_score: 0.175926\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[30]\tvalid_0's binary_logloss: 0.383591\tvalid_0's f1_score: 0.161774\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[54]\tvalid_0's binary_logloss: 0.375838\tvalid_0's f1_score: 0.196658\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[57]\tvalid_0's binary_logloss: 0.363023\tvalid_0's f1_score: 0.191535\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[125]\tvalid_0's binary_logloss: 0.297465\tvalid_0's f1_score: 0.203369\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[50]\tvalid_0's binary_logloss: 0.342689\tvalid_0's f1_score: 0.197196\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[496]\tvalid_0's binary_logloss: 0.328034\tvalid_0's f1_score: 0.205459\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[126]\tvalid_0's binary_logloss: 0.305375\tvalid_0's f1_score: 0.207163\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[139]\tvalid_0's binary_logloss: 0.308634\tvalid_0's f1_score: 0.210011\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[258]\tvalid_0's binary_logloss: 0.280914\tvalid_0's f1_score: 0.205094\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[90]\tvalid_0's binary_logloss: 0.326948\tvalid_0's f1_score: 0.214356\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                 \n",
      "[84]\tvalid_0's binary_logloss: 0.321355\tvalid_0's f1_score: 0.210367\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[75]\tvalid_0's binary_logloss: 0.320083\tvalid_0's f1_score: 0.203046\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[87]\tvalid_0's binary_logloss: 0.285703\tvalid_0's f1_score: 0.208443\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[47]\tvalid_0's binary_logloss: 0.37145\tvalid_0's f1_score: 0.2\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[241]\tvalid_0's binary_logloss: 0.28945\tvalid_0's f1_score: 0.200639\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[285]\tvalid_0's binary_logloss: 0.298639\tvalid_0's f1_score: 0.205882\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[56]\tvalid_0's binary_logloss: 0.346752\tvalid_0's f1_score: 0.204082\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[101]\tvalid_0's binary_logloss: 0.360102\tvalid_0's f1_score: 0.203603\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[188]\tvalid_0's binary_logloss: 0.293501\tvalid_0's f1_score: 0.20214\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[23]\tvalid_0's binary_logloss: 0.360827\tvalid_0's f1_score: 0.200334\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[57]\tvalid_0's binary_logloss: 0.370218\tvalid_0's f1_score: 0.194747\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[48]\tvalid_0's binary_logloss: 0.337164\tvalid_0's f1_score: 0.208583\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[195]\tvalid_0's binary_logloss: 0.333817\tvalid_0's f1_score: 0.189282\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[287]\tvalid_0's binary_logloss: 0.302289\tvalid_0's f1_score: 0.209557\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[116]\tvalid_0's binary_logloss: 0.302262\tvalid_0's f1_score: 0.20651\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[29]\tvalid_0's binary_logloss: 0.593695\tvalid_0's f1_score: 0.177446\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[117]\tvalid_0's binary_logloss: 0.333985\tvalid_0's f1_score: 0.20258\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[105]\tvalid_0's binary_logloss: 0.294929\tvalid_0's f1_score: 0.20015\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[46]\tvalid_0's binary_logloss: 0.316883\tvalid_0's f1_score: 0.201815\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[64]\tvalid_0's binary_logloss: 0.315763\tvalid_0's f1_score: 0.2129\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[74]\tvalid_0's binary_logloss: 0.352841\tvalid_0's f1_score: 0.202697\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[52]\tvalid_0's binary_logloss: 0.329412\tvalid_0's f1_score: 0.201723\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[92]\tvalid_0's binary_logloss: 0.298683\tvalid_0's f1_score: 0.20739\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[140]\tvalid_0's binary_logloss: 0.303948\tvalid_0's f1_score: 0.200127\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[102]\tvalid_0's binary_logloss: 0.327163\tvalid_0's f1_score: 0.20099\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[93]\tvalid_0's binary_logloss: 0.307925\tvalid_0's f1_score: 0.197148\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[54]\tvalid_0's binary_logloss: 0.314731\tvalid_0's f1_score: 0.203304\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[59]\tvalid_0's binary_logloss: 0.31909\tvalid_0's f1_score: 0.195975\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[172]\tvalid_0's binary_logloss: 0.320091\tvalid_0's f1_score: 0.202519\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[21]\tvalid_0's binary_logloss: 0.349504\tvalid_0's f1_score: 0.201112\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[201]\tvalid_0's binary_logloss: 0.342734\tvalid_0's f1_score: 0.199366\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[242]\tvalid_0's binary_logloss: 0.337676\tvalid_0's f1_score: 0.202375\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[74]\tvalid_0's binary_logloss: 0.302879\tvalid_0's f1_score: 0.201359\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[162]\tvalid_0's binary_logloss: 0.349758\tvalid_0's f1_score: 0.204907\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[165]\tvalid_0's binary_logloss: 0.286337\tvalid_0's f1_score: 0.208881\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[71]\tvalid_0's binary_logloss: 0.325377\tvalid_0's f1_score: 0.214423\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[140]\tvalid_0's binary_logloss: 0.312718\tvalid_0's f1_score: 0.200531\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[139]\tvalid_0's binary_logloss: 0.298309\tvalid_0's f1_score: 0.205419\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[30]\tvalid_0's binary_logloss: 0.366431\tvalid_0's f1_score: 0.186335\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[109]\tvalid_0's binary_logloss: 0.304732\tvalid_0's f1_score: 0.198659\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[45]\tvalid_0's binary_logloss: 0.332081\tvalid_0's f1_score: 0.204556\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[99]\tvalid_0's binary_logloss: 0.308008\tvalid_0's f1_score: 0.207317\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[87]\tvalid_0's binary_logloss: 0.327179\tvalid_0's f1_score: 0.203963\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[65]\tvalid_0's binary_logloss: 0.310377\tvalid_0's f1_score: 0.198805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[73]\tvalid_0's binary_logloss: 0.367495\tvalid_0's f1_score: 0.19656\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[86]\tvalid_0's binary_logloss: 0.300533\tvalid_0's f1_score: 0.197688\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[118]\tvalid_0's binary_logloss: 0.304243\tvalid_0's f1_score: 0.20339\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[119]\tvalid_0's binary_logloss: 0.332339\tvalid_0's f1_score: 0.207026\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[54]\tvalid_0's binary_logloss: 0.334708\tvalid_0's f1_score: 0.198775\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[128]\tvalid_0's binary_logloss: 0.295473\tvalid_0's f1_score: 0.205988\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[295]\tvalid_0's binary_logloss: 0.29279\tvalid_0's f1_score: 0.203135\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[99]\tvalid_0's binary_logloss: 0.282282\tvalid_0's f1_score: 0.19816\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[114]\tvalid_0's binary_logloss: 0.289909\tvalid_0's f1_score: 0.197617\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[141]\tvalid_0's binary_logloss: 0.289569\tvalid_0's f1_score: 0.203346\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[82]\tvalid_0's binary_logloss: 0.368927\tvalid_0's f1_score: 0.202461\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[103]\tvalid_0's binary_logloss: 0.309971\tvalid_0's f1_score: 0.206086\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[196]\tvalid_0's binary_logloss: 0.331857\tvalid_0's f1_score: 0.20357\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[113]\tvalid_0's binary_logloss: 0.314993\tvalid_0's f1_score: 0.187575\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[55]\tvalid_0's binary_logloss: 0.311861\tvalid_0's f1_score: 0.197706\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[219]\tvalid_0's binary_logloss: 0.293052\tvalid_0's f1_score: 0.209211\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[108]\tvalid_0's binary_logloss: 0.351273\tvalid_0's f1_score: 0.206867\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[77]\tvalid_0's binary_logloss: 0.30574\tvalid_0's f1_score: 0.205431\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[50]\tvalid_0's binary_logloss: 0.340453\tvalid_0's f1_score: 0.198688\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[155]\tvalid_0's binary_logloss: 0.342308\tvalid_0's f1_score: 0.195702\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[41]\tvalid_0's binary_logloss: 0.338511\tvalid_0's f1_score: 0.202602\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[303]\tvalid_0's binary_logloss: 0.291321\tvalid_0's f1_score: 0.197993\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[36]\tvalid_0's binary_logloss: 0.319524\tvalid_0's f1_score: 0.201881\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[205]\tvalid_0's binary_logloss: 0.343238\tvalid_0's f1_score: 0.201128\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[78]\tvalid_0's binary_logloss: 0.302218\tvalid_0's f1_score: 0.205682\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[178]\tvalid_0's binary_logloss: 0.28913\tvalid_0's f1_score: 0.198138\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[211]\tvalid_0's binary_logloss: 0.353562\tvalid_0's f1_score: 0.204186\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[82]\tvalid_0's binary_logloss: 0.338199\tvalid_0's f1_score: 0.20611\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[100]\tvalid_0's binary_logloss: 0.311491\tvalid_0's f1_score: 0.206152\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[398]\tvalid_0's binary_logloss: 0.327388\tvalid_0's f1_score: 0.197296\n",
      "Training until validation scores don't improve for 100 rounds                      \n",
      "Early stopping, best iteration is:                                                 \n",
      "[124]\tvalid_0's binary_logloss: 0.335715\tvalid_0's f1_score: 0.20339\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[231]\tvalid_0's binary_logloss: 0.325559\tvalid_0's f1_score: 0.209742\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[48]\tvalid_0's binary_logloss: 0.32313\tvalid_0's f1_score: 0.204975\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[98]\tvalid_0's binary_logloss: 0.297185\tvalid_0's f1_score: 0.198261\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[78]\tvalid_0's binary_logloss: 0.296375\tvalid_0's f1_score: 0.196581\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[68]\tvalid_0's binary_logloss: 0.348629\tvalid_0's f1_score: 0.199639\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[75]\tvalid_0's binary_logloss: 0.29279\tvalid_0's f1_score: 0.196429\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[124]\tvalid_0's binary_logloss: 0.338483\tvalid_0's f1_score: 0.203189\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[58]\tvalid_0's binary_logloss: 0.308316\tvalid_0's f1_score: 0.189112\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[79]\tvalid_0's binary_logloss: 0.302867\tvalid_0's f1_score: 0.198394\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[258]\tvalid_0's binary_logloss: 0.300684\tvalid_0's f1_score: 0.213011\n",
      "Training until validation scores don't improve for 100 rounds                       \n",
      "Early stopping, best iteration is:                                                  \n",
      "[375]\tvalid_0's binary_logloss: 0.38624\tvalid_0's f1_score: 0.203779\n",
      " 37%|███▋      | 111/300 [06:46<11:32,  3.66s/trial, best loss: -0.2144230769230769]\n",
      "Best hyperparameters: {'colsample_bytree': 0.933211141656504, 'learning_rate': 0.09784288222891396, 'max_depth': 22.0, 'min_child_weight': 6.0, 'num_leaves': 144.0, 'subsample': 0.7379599291575736}\n"
     ]
    }
   ],
   "source": [
    "# Trials 객체로 결과 기록\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "trials = Trials()\n",
    "# 최적화 수행\n",
    "best = fmin(fn=objective,\n",
    "            space=space,\n",
    "            algo=tpe.suggest,\n",
    "            max_evals=300,\n",
    "            trials=trials,\n",
    "            rstate=np.random.default_rng(SEED),\n",
    "            early_stop_fn=no_progress_loss(50))\n",
    "\n",
    "# 최적 파라미터 출력\n",
    "print(\"Best hyperparameters:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c3de598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.933211141656504,\n",
       " 'learning_rate': 0.09784288222891396,\n",
       " 'max_depth': 22,\n",
       " 'min_child_weight': 6,\n",
       " 'num_leaves': 144,\n",
       " 'subsample': 0.7379599291575736,\n",
       " 'n_estimators': 1000}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best['max_depth'] = int(best['max_depth'])\n",
    "best['num_leaves'] = int(best['num_leaves'])\n",
    "best['min_child_weight'] = int(best['min_child_weight'])\n",
    "best['n_estimators'] = 1000\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "062f23da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[71]\tvalid_0's binary_logloss: 0.325377\tvalid_0's f1_score: 0.214423\n",
      "0.2144230769230769\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMClassifier(**best, class_weight = class_weights, random_state=SEED, verbose=-1)\n",
    "model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], eval_metric=f1_eval_metric,\n",
    "          categorical_feature = cat_features,\n",
    "          callbacks = [lgb.early_stopping(stopping_rounds = 100), lgb.log_evaluation(period = 0)]\n",
    "         )\n",
    "\n",
    "preds = model.predict(x_valid)\n",
    "score = f1_score(y_valid, preds)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8253fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop('target', axis=1)\n",
    "y_train = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1ac8d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5307946325610651, 1: 8.618297872340426}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class weight\n",
    "SEED = 0\n",
    "n_fold = 5\n",
    "class_weights = compute_class_weight(classes=[0, 1], y=train['target'], class_weight='balanced')\n",
    "class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "920c5137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1 번째 fold ==================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[151]\tvalid_0's binary_logloss: 0.287931\tvalid_0's f1_score: 0.208494\n",
      "\n",
      " 2 번째 fold ==================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[203]\tvalid_0's binary_logloss: 0.289857\tvalid_0's f1_score: 0.215645\n",
      "\n",
      " 3 번째 fold ==================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[187]\tvalid_0's binary_logloss: 0.291141\tvalid_0's f1_score: 0.214894\n",
      "\n",
      " 4 번째 fold ==================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[92]\tvalid_0's binary_logloss: 0.320262\tvalid_0's f1_score: 0.233533\n",
      "\n",
      " 5 번째 fold ==================================================\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[91]\tvalid_0's binary_logloss: 0.317778\tvalid_0's f1_score: 0.221548\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "model_scores = [] # model에서 뽑아내는 score\n",
    "models = []\n",
    "val_scores = [] # f1 score로 측정한 validation score\n",
    "best_thresholds = []\n",
    "pred_list = []\n",
    "\n",
    "num = 0\n",
    "cv = StratifiedKFold(n_splits = n_fold , shuffle=True, random_state = SEED)\n",
    "\n",
    "for train_idx, valid_idx in cv.split(x_train, y_train):\n",
    "    num += 1\n",
    "    scores = []\n",
    "    print('\\n', num, '번째 fold', \"=\"*50)\n",
    "    # train valid split\n",
    "    x_train_cv, x_valid_cv, y_train_cv, y_valid_cv = x_train.iloc[train_idx], x_train.iloc[valid_idx], y_train.iloc[train_idx], y_train.iloc[valid_idx]\n",
    "    # model fitting\n",
    "    model = lgb.LGBMClassifier(**best, class_weight = class_weights, random_state=SEED, verbose=-1)\n",
    "    model.fit(x_train_cv, y_train_cv, eval_set=[(x_valid_cv, y_valid_cv)],\n",
    "            eval_metric=f1_eval_metric, categorical_feature = cat_features,\n",
    "            callbacks = [lgb.early_stopping(stopping_rounds = 100), lgb.log_evaluation(period = 0)]\n",
    "             )\n",
    "    models.append(model)\n",
    "\n",
    "    # validation evaluate\n",
    "    val_proba = model.predict_proba(x_valid_cv)[:, 1]\n",
    "    # ROC Curve 계산\n",
    "    fpr, tpr, thresholds = roc_curve(y_valid_cv, val_proba)\n",
    "    f1_scores = []\n",
    "    for thresh in thresholds:\n",
    "        y_pred = [1 if y >= thresh else 0 for y in val_proba]\n",
    "        f1 = f1_score(y_valid_cv, y_pred)\n",
    "        f1_scores.append(f1)\n",
    "    threshold = thresholds[np.argmax(f1_scores)]\n",
    "    best_score = max(f1_scores)\n",
    "    best_thresholds.append(threshold)\n",
    "    val_scores.append(best_score) # f1 score로 측정한 validation score\n",
    "    #best_thresholds.append(thresholds[index])\n",
    "\n",
    "    # final prediction\n",
    "    pred = model.predict_proba(test.drop('target',axis=1))[:, 1]\n",
    "    pred_list.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dd0069ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4766669920134474,\n",
       " 0.5048609521529821,\n",
       " 0.7308354154020255,\n",
       " 0.621873939446824,\n",
       " 0.6249324261928532]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "29d5ae1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5918339450416265"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(best_thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a087628a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cv f1 mean score : 0.22415407141191226\n",
      "test에서 1로 예측된 수: 619\n"
     ]
    }
   ],
   "source": [
    "# 이거 또는 아래 코드로 예측하는데, 제출은 아래 코드로 했음.\n",
    "\n",
    "final_pred = np.where(sum(pred_list)/n_fold >= np.mean(best_thresholds), 1, 0)\n",
    "print('cv f1 mean score :', np.mean(val_scores))\n",
    "print('test에서 1로 예측된 수:', sum(final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44e97214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6bd23b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test에서 1로 예측된 수: 627\n",
      "test에서 0로 예측된 수: 16734\n"
     ]
    }
   ],
   "source": [
    "#후보군. 이거 제출 안함.\n",
    "\n",
    "binary_predictions = []\n",
    "for i in range(len(pred_list)):\n",
    "    binary_pred = (pred_list[i] >= best_thresholds[i]).astype(int)\n",
    "    binary_predictions.append(binary_pred)\n",
    "binary_predictions = np.array(binary_predictions)\n",
    "final_pred = (np.sum(binary_predictions, axis=0) >= 3).astype(int)\n",
    "print('test에서 1로 예측된 수:', sum(final_pred))\n",
    "print('test에서 0로 예측된 수:', len(final_pred) - sum(final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "88e165a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "164"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 pred_list 배열에 threshold 적용\n",
    "binary_preds = [pred > thr for pred, thr in zip(pred_list, best_thresholds)]\n",
    "\n",
    "# 모두 다른 항목 수 계산\n",
    "np.sum(np.all(binary_preds, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e62e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "all_importances = []\n",
    "\n",
    "# Compute permutation importance for each model on its corresponding validation set\n",
    "for model, (train_idx, valid_idx) in zip(models, cv.split(x_train, y_train)):\n",
    "    x_valid_cv, y_valid_cv = x_train.iloc[valid_idx], y_train.iloc[valid_idx]\n",
    "    \n",
    "    # Permutation importance with F1 score as the evaluation metric\n",
    "    result = permutation_importance(model, x_valid_cv, y_valid_cv, scoring='f1', n_repeats=10, random_state=SEED)\n",
    "    all_importances.append(result.importances_mean)  # Take the mean importance across the repeats\n",
    "\n",
    "# Convert list to numpy array for easier averaging\n",
    "all_importances = np.array(all_importances)\n",
    "\n",
    "# Average the importances across all models\n",
    "mean_importances = np.mean(all_importances, axis=0)\n",
    "\n",
    "# Create a DataFrame for easier visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': x_train_cv.columns,\n",
    "    'Importance': mean_importances\n",
    "})\n",
    "\n",
    "# Sort by importance and select top 30 features\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True).iloc[:30]\n",
    "\n",
    "# Visualize the top 30 feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Feature Importance (Averaged Permutation Importance)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc124b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame for easier visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': x_train_cv.columns,\n",
    "    'Importance': mean_importances\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Sort by importance and select top 30 features\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False).reset_index(drop=True).iloc[-30:-1]\n",
    "\n",
    "# Visualize the top 30 feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Feature Importance (Averaged Permutation Importance)\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9178ae56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                       Feature  Importance\n",
      "0                                                Equipment_Dam   -0.000122\n",
      "1                                             Model.Suffix_Dam    0.000010\n",
      "2                                                Workorder_Dam    0.098928\n",
      "3                                       Model.Suffix_AutoClave    0.000251\n",
      "4                                          Workorder_AutoClave    0.003505\n",
      "5                          Chamber Temp. Judge Value_AutoClave    0.000725\n",
      "6                                              Equipment_Fill1   -0.000233\n",
      "7                                           Model.Suffix_Fill1    0.000008\n",
      "8                                              Workorder_Fill1   -0.000438\n",
      "9                                              Equipment_Fill2   -0.000142\n",
      "10                                          Model.Suffix_Fill2    0.000000\n",
      "11                                             Workorder_Fill2    0.001432\n",
      "12                                                       Dam X    0.000137\n",
      "13                                                       Dam Y    0.000700\n",
      "14                                                       Dam Z    0.000318\n",
      "15                                                     Fill1 X   -0.000312\n",
      "16                                                     Fill1 Y    0.000126\n",
      "17                                                     Fill1 Z    0.000045\n",
      "18                                                     Fill2 X   -0.000009\n",
      "19                                                     Fill2 Y    0.000003\n",
      "20                                                     Fill2 Z   -0.000096\n",
      "21            Head Clean&Purge Position X Collect Result_Fill1    0.000012\n",
      "22            Head Clean&Purge Position Y Collect Result_Fill1    0.000049\n",
      "23            Head Clean&Purge Position Z Collect Result_Fill1   -0.000154\n",
      "24              Head Clean&Purge Position X Collect Result_Dam    0.000317\n",
      "25              Head Clean&Purge Position Y Collect Result_Dam    0.000000\n",
      "26              Head Clean&Purge Position Z Collect Result_Dam    0.000000\n",
      "27                         CURE  POSITION X Collect Result_Dam   -0.000037\n",
      "28                         CURE  POSITION Θ Collect Result_Dam    0.000000\n",
      "29                       CURE  POSITION X Collect Result_Fill2    0.000000\n",
      "30                       CURE  POSITION Z Collect Result_Fill2    0.000034\n",
      "31     HEAD NORMAL COORDINATE X AXIS(Stage) Collect Result_Dam   -0.000412\n",
      "32     HEAD NORMAL COORDINATE Y AXIS(Stage) Collect Result_Dam    0.000281\n",
      "33     HEAD NORMAL COORDINATE Z AXIS(Stage) Collect Result_Dam    0.000123\n",
      "34   HEAD NORMAL COORDINATE X AXIS(Stage) Collect Result_Fill1   -0.000119\n",
      "35   HEAD NORMAL COORDINATE Y AXIS(Stage) Collect Result_Fill1   -0.000035\n",
      "36   HEAD NORMAL COORDINATE Z AXIS(Stage) Collect Result_Fill1    0.000349\n",
      "37   HEAD NORMAL COORDINATE X AXIS(Stage) Collect Result_Fill2    0.000000\n",
      "38   HEAD NORMAL COORDINATE Y AXIS(Stage) Collect Result_Fill2    0.000000\n",
      "39   HEAD NORMAL COORDINATE Z AXIS(Stage) Collect Result_Fill2    0.000000\n",
      "40     HEAD NORMAL COORDINATE AXIS(Stage1) Judge Value_Dam XYZ   -0.000146\n",
      "41                   Head Zero Position Collect Result_Dam XYZ   -0.000005\n",
      "42                                            head_interaction    0.015012\n",
      "43                                                stage123_dam    0.001379\n",
      "44                                             clean_purge_dam    0.000000\n",
      "45                                              stage123_fill1    0.000782\n",
      "46                                           clean_purge_fill1    0.000236\n",
      "47                                              stage123_fill2    0.000000\n",
      "48                                           clean_purge_fill2    0.000000\n",
      "49                                            cure_interaction    0.000041\n",
      "50                                               start_end_dam    0.000000\n",
      "51                                             start_end_fill2    0.000000\n",
      "52                                          Mismatch_Dam_Fill1    0.000014\n",
      "53                                          Mismatch_Dam_Fill2    0.012989\n",
      "54                                        Mismatch_Fill1_Fill2    0.000000\n",
      "55                                   Mismatch_Receip_Dam_Fill1    0.000003\n",
      "56                                   Mismatch_Receip_Dam_Fill2    0.000021\n",
      "57                                 Mismatch_Receip_Fill1_Fill2    0.000000\n",
      "58                               CURE SPEED Collect Result_Dam   -0.000009\n",
      "59                DISCHARGED SPEED OF RESIN Collect Result_Dam    0.000179\n",
      "60         DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam    0.001157\n",
      "61         DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam    0.001116\n",
      "62         DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam   -0.001524\n",
      "63                  Dispense Volume(Stage1) Collect Result_Dam    0.001446\n",
      "64                  Dispense Volume(Stage2) Collect Result_Dam    0.001569\n",
      "65                  Dispense Volume(Stage3) Collect Result_Dam   -0.000651\n",
      "66                        Machine Tact time Collect Result_Dam    0.000724\n",
      "67                                 PalletID Collect Result_Dam    0.001076\n",
      "68                           Production Qty Collect Result_Dam    0.005471\n",
      "69                                Receip No Collect Result_Dam    0.008705\n",
      "70            Stage1 Circle1 Distance Speed Collect Result_Dam    0.000000\n",
      "71            Stage1 Circle2 Distance Speed Collect Result_Dam   -0.000223\n",
      "72              Stage1 Line1 Distance Speed Collect Result_Dam    0.000000\n",
      "73              Stage1 Line2 Distance Speed Collect Result_Dam    0.000000\n",
      "74              Stage1 Line4 Distance Speed Collect Result_Dam   -0.000026\n",
      "75            Stage2 Circle1 Distance Speed Collect Result_Dam    0.000000\n",
      "76            Stage2 Circle2 Distance Speed Collect Result_Dam    0.000025\n",
      "77              Stage2 Line2 Distance Speed Collect Result_Dam    0.000000\n",
      "78              Stage2 Line3 Distance Speed Collect Result_Dam    0.000000\n",
      "79              Stage2 Line4 Distance Speed Collect Result_Dam    0.000009\n",
      "80            Stage3 Circle2 Distance Speed Collect Result_Dam    0.000000\n",
      "81              Stage3 Line2 Distance Speed Collect Result_Dam    0.000000\n",
      "82              Stage3 Line4 Distance Speed Collect Result_Dam    0.000000\n",
      "83                              THICKNESS 1 Collect Result_Dam    0.000000\n",
      "84                              THICKNESS 2 Collect Result_Dam    0.000000\n",
      "85                              THICKNESS 3 Collect Result_Dam    0.000000\n",
      "86                                 WorkMode Collect Result_Dam    0.000000\n",
      "87                       1st Pressure Collect Result_AutoClave    0.001798\n",
      "88               1st Pressure 1st Pressure Unit Time_AutoClave    0.000497\n",
      "89                       2nd Pressure Collect Result_AutoClave    0.001706\n",
      "90                            2nd Pressure Unit Time_AutoClave   -0.000043\n",
      "91                       3rd Pressure Collect Result_AutoClave    0.002619\n",
      "92                            3rd Pressure Unit Time_AutoClave    0.004111\n",
      "93                      Chamber Temp. Collect Result_AutoClave    0.002118\n",
      "94              DISCHARGED SPEED OF RESIN Collect Result_Fill1    0.000051\n",
      "95       DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1   -0.000611\n",
      "96       DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1    0.003739\n",
      "97       DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1    0.003295\n",
      "98                Dispense Volume(Stage1) Collect Result_Fill1    0.001536\n",
      "99                Dispense Volume(Stage2) Collect Result_Fill1    0.000944\n",
      "100               Dispense Volume(Stage3) Collect Result_Fill1    0.000849\n",
      "101                     Machine Tact time Collect Result_Fill1    0.001037\n",
      "102                              PalletID Collect Result_Fill1    0.003556\n",
      "103                        Production Qty Collect Result_Fill1    0.003548\n",
      "104                             Receip No Collect Result_Fill1    0.001731\n",
      "105                              WorkMode Collect Result_Fill1    0.000000\n",
      "106                            CURE SPEED Collect Result_Fill2    0.000118\n",
      "107                     Machine Tact time Collect Result_Fill2   -0.003473\n",
      "108                              PalletID Collect Result_Fill2   -0.001725\n",
      "109                        Production Qty Collect Result_Fill2    0.003508\n",
      "110                             Receip No Collect Result_Fill2   -0.000191\n",
      "111                              WorkMode Collect Result_Fill2    0.000000\n"
     ]
    }
   ],
   "source": [
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': x_train_cv.columns,\n",
    "    'Importance': mean_importances\n",
    "})\n",
    "\n",
    "print(feature_importance_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "bbb67fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "todrop = feature_importance_df[feature_importance_df.Importance <0].Feature.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adf8300",
   "metadata": {},
   "source": [
    "## 4. 제출하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88e37a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출 데이터 읽어오기 (df_test는 전처리된 데이터가 저장됨)\n",
    "df_sub = pd.read_csv(\"submission.csv\")\n",
    "df_sub[\"target\"] = final_pred\n",
    "df_sub[\"target\"] = df_sub[\"target\"].apply(lambda x : 'AbNormal' if x==1 else 'Normal')\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7867ce",
   "metadata": {},
   "source": [
    "**우측 상단의 제출 버튼을 클릭해 결과를 확인하세요**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
